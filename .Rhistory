library(ROCR)
perf <- performance(prediction(pred2, bn[-t.id,"class"]),
"tpr",
"fpr")
plot(perf)
install.packages("MASS")
library(MASS)
library(caret)
bn <- read.csv("Seccion 05 - El proceso de clasificacion/banknote-authentication.csv")
View(bn)
bn$class <- factor(bn$class)
set.seed(2018)
t.id <- createDataPartition(bn$class,
p = 0.7,
list = F)
mod <- lda(bn[t.id,1:4],
bn[t.id,5])
bn[t.id,"predic"] <- predict(mod,
bn[t.id,1:4])$class
table(bn[t.id, "class"],
bn[t.id, "predic"],
dnn = c("actual", "predichos"))
bn[-t.id,"predic"] <- predict(mod,
bn[-t.id,1:4])$class
table(bn[t.id, "class"],
bn[t.id, "predic"],
dnn = c("actual", "predichos"))
table(bn[-t.id, "class"],
bn[-t.id, "predic"],
dnn = c("actual", "predichos"))
# el de abajo es identico al de arriba
mod <- lda(class ~ .,
data = bn[t.id,])
library(caret)
bh <- read.csv("Seccion 05 - El proceso de clasificacion/boston-housing-logistic.csv")
View(bh)
bh$CLASS <- factor(bh$CLASS)
View(bh)
set.seed(2018)
mod <- glm(CLASS ~ .,
data = bh[t.id,],
family = binomial)
t.id <- createDataPartition(bh$CLASS,
p = 0.7,
list = F)
mod <- glm(CLASS ~ .,
data = bh[t.id,],
family = binomial)
summary(mod)
bh[-t.id, "predic"] <- predict(mod,
newdata = bh[-t.id],
type = "response")
bh[-t.id, "predic"] <- predict(mod,
newdata = bh[-t.id,],
type = "response")
bh[-t.id, "predic"] <- NULL
bh["predic"] <- NULL
bh[-t.id, "prob_success"] <- predict(mod,
newdata = bh[-t.id,],
type = "response")
bh[-t.id, "pred_50"] <- ifelse(bh[-t.id,"prob_success"] > 0.5, 1, 0)
table(bh[-t.id,"CLASS"],
bh[-t.id, "prob_50"],
dnn = c("Actual", "Predicho"))
table(bh[-t.id,"CLASS"],
bh[-t.id, "pred_50"],
dnn = c("Actual", "Predicho"))
d <- read.csv("Seccion 06 - 75. La raíz del error cuadrático medio/rmse.csv")
View(d)
sqrt(mean((d$price - d$pred)^2))
rmse <- sqrt(mean((d$price - d$pred)^2))
rmse
plot(d$price,
d$pred,
xlab = "Actual",
ylab = "Predicho")
abline(0,1)
rmse.f <- function(actual, predicted){
return(sqrt(mean((actual - predicted)^2)))
}
rmse.f(d$price, d$pred)
install.packages"FNN")
install.packages("FNN")
library(dummies)
library(FNN)
library(scales)
library(caret)
edu <- read.csv("Seccion 06 - Regresion/education.csv")
View(edu)
#variables dummies para region
dms <- dummy(edu$region, sep = "_")
View(dms)
cbind(edu,dms)
edu <- cbind(edu,dms)
View(edu)
#normalizamos..
edu$urban.s <- rescale(edu$urban)
edu$income.s <- rescale(edu$income)
edu$under18.s <- rescale(edu$under18)
set.seed(2018)
t.id <- createDataPartition(edu$expense,
p = 0.6,
list = F)
tr <- edu[t.id,]
temp <- edu[-t.id,]
v.id <- createDataPartition(temp$expense,
p = 0.5,
list = F)
val <- temp[v.id, ]
test <- temp[-v.id, ]
#Creamos varios modelos para ver cual es el mejor
reg1 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 1,
algorithm = "brute")
rmse1 <- sqrt(mean((reg1$pred - val$expense)^2))
rmse1
## k = 2
reg2 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 2,
algorithm = "brute")
rmse2 <- sqrt(mean((reg1$pred - val$expense)^2))
rmse2
## k = 1
reg1 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 1,
algorithm = "brute")
rmse1 <- sqrt(mean((reg1$pred - val$pred)^2))
rmse1
rmse1 <- sqrt(mean((reg1$pred - val$expense)^2))
rmse1
## k = 2
reg2 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 2,
algorithm = "brute")
rmse2 <- sqrt(mean((reg2$pred - val$expense)^2))
rmse2
## k = 3
reg3 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 3,
algorithm = "brute")
rmse3 <- sqrt(mean((reg3$pred - val$expense)^2))
rmse3
## k = 4
reg4 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 4,
algorithm = "brute")
rmse4 <- sqrt(mean((reg4$pred - val$expense)^2))
rmse4
errors = c(rmse1,rmse2,rmse3,rmse4)
plot(errors,
type = 'o',
xlab = "k",
ylab = "RSME")
## k = 1
reg1 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 1,
algorithm = "brute")
rmse1 <- sqrt(mean((val$expense - reg1$pred)^2))
rmse1
## k = 1
reg1 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 1,
algorithm = "brute")
rmse1 <- sqrt(mean((reg1$pred - val$expense)^2))
rmse1
#######################
reg.test <- knn.reg(tr[,7:12],
test[,7:12],
tr$expense,
k = 4,
algorithm = "brute")
rmse.test <- sqrt(mean((reg.test$pred - test$expense)^2))
rmse.test
#######################
reg.test <- knn.reg(tr[,7:12],
test[,7:12],
tr$expense,
k = 3,
algorithm = "brute")
rmse.test <- sqrt(mean((reg.test$pred - test$expense)^2))
rmse.test
#######################
reg.test <- knn.reg(tr[,7:12],
test[,7:12],
tr$expense,
k = 4,
algorithm = "brute")
rmse.test <- sqrt(mean((reg.test$pred - test$expense)^2))
rmse.test
t.id <- createDataPartition(edu$expense,
p = 0.7,
list = F)
tr <- edu[t.id, ]
val <- edu[-t.id, ]
reg <- knn.reg(tr[,7:12],
test = NULL,
tr$expense,
k = 4,
algorithm = "brute")
rmse <- sqrt(mean((reg$residuals)^2))
rmse
reg <- knn.reg(tr[,7:12],
test = NULL,
tr$expense,
k = 3,
algorithm = "brute")
rmse <- sqrt(mean((reg$residuals)^2))
rmse
reg <- knn.reg(tr[,7:12],
test = NULL,
tr$expense,
k = 4,
algorithm = "brute")
rmse <- sqrt(mean((reg$residuals)^2))
rmse
library(FNN)
##################################################
# creamos un fx para hacer las regresiones
##################################################
rdacb.knn.reg <- function(tr_predictors, val_predictors,
tr_target, val_target, k){
library(FNN)
res <- knn.reg(tr_predictors,
val_predictors,
tr_target,
k,
algorithm = "brute")
rmserror <- sqrt(mean((val_target-res$pred)^2))
cat(paste("RMSE para k =  ", toString(k) , ": ", rmserror, "\n", sep = ""))
}
rdacb.knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
1)
rdacb.knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
2)
rdacb.knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
3)
rdacb.knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
4)
rdacb.knn.multi <- function(tr_predictors, val_predictors,
tr_target, val_target, start_k, end_k){
rms_errors <- vector()
for(k in start_k:end_k){
rms_error <- rdacb.knn.reg(tr_predictors,
val_predictors,
tr_target,
val_target,
k)
rms_errors <- c(rms_errors, rms_error)
}
plot(rms_errors,
type = "o",
xlab = "k",
ylab = "RMSE")
}
rdacb.knn.multi(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
1,
5)
##################################################
# creamos un fx para hacer las regresiones
##################################################
rdacb.knn.reg <- function(tr_predictors, val_predictors,
tr_target, val_target, k){
library(FNN)
res <- knn.reg(tr_predictors,
val_predictors,
tr_target,
k,
algorithm = "brute")
rmserror <- sqrt(mean((val_target-res$pred)^2))
cat(paste("RMSE para k =  ", toString(k) , ": ", rmserror, "\n", sep = ""))
rmserror
}
rdacb.knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
1)
rdacb.knn.multi(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
1,
5)
rdacb.knn.multi(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
1,
10)
df = data.frame(actual = test$expense,
pred = reg.test$pred)
View(df)
plt(df)
plot(df)
abline(0,1)
library(caret)
auto <-read.csv("Seccion 06 - Regresion/auto-mpg.csv")
View(auto)
auto$cylinders <- factor(auto$cylinders,
levels = c(3,5,6,8),
labels = c("3 cyl", "4 cyl", "5 cyl", "6 cyl", "8 cyl"))
auto$cylinders <- factor(auto$cylinders,
levels = c(3,4, 5,6,8),
labels = c("3 cyl", "4 cyl", "5 cyl", "6 cyl", "8 cyl"))
set.seed(2018)
t.id <- createDataPartition(auto$mpg,
p = 0.7,
list = F)
names(auto)
mod <- lm(mpg ~ .,
data = auto[t.id, -c(1, 8, 9)])
View(auto)
View(mod)
mod
summary(mod)
boxplot(mod$residuals)
sqrt(mean((mod$fitted.values - auto[t.id,]$mpg)^2))
sqrt(mean((mod$fitted.values - auto[t.id,]$mpg)^2))
pred <- predict(mod, auto[-t.id, -c(1,8,9)])
sqrt(mean((pred - auto[-t.id,]$mpg)^2))
pred
auto[-t.id,]$pred <- pred
auto$pred[-t.id,] <- pred
auto2 <- pred[-t.id,]
auto2 <- auto[-t.id,]
auto2$pred <- pred
View(auto2)
par(mfrow=c(2,2))
plot(mod)
auto <- within(auto, cylinders <- relevel(cylinders, ref="4 cyl"))
mod <- lm(mpg ~ .,
data = auto[t.id, -c(1, 8, 9)])
mod
pred <- predict(mod, auto[-t.id, -c(1,8,9)])
sqrt(mean((pred - auto[-t.id,]$mpg)^2))
plot(mod)
auto2$pred2 <- pred
#############################################################
# 81. La función step para simplificar el modelo lineal
#############################################################
library(MASS)
mod
summary(mod)
step.model <- stepAIC(mod, direction = "forward")
summary(step.model)
step.model <- stepAIC(mod, direction = "backward")
View(step.model)
install.packages(c("curl", "knitr", "recipes", "tidyr", "zip"))
install.packages("curl")
install.packages("curl")
install.packages("curl")
library(curl)
detach("package:curl", unload = TRUE)
install.packages("curl")
install.packages("rpart")
library(rpart)
install.packages("rpart.plot")
library(rpart.plot)
library(caret)
bh <- read.csv("Seccion 06 - Regresion/BostonHousing.csv")
View(bh)
t.id <- createDataPartition(bh$MEDV,
p = 0.7,
list = F)
bfit <- rpart(MEDV ~ .,
data = bh[t.id,])
bfit
prp(bfit,
type = 2,
nn = T,
fallen.leaves = T,
faclen = 4,
varlen = 8,
shadow.col = "gray")
bfit$cptable
plotcp(bfit)
0.2642192 + 0.04714776
bfitpruned <- prune(bfit, cp = 0.311367)
prp(bfitpruned,
type = 2,
nn = T,
fallen.leaves = T,
faclen = 4,
varlen = 8,
shadow.col = "gray")
bfitpruned <- prune(bfit, cp = 0.02357120)
prp(bfitpruned,
type = 2,
nn = T,
fallen.leaves = T,
faclen = 4,
varlen = 8,
shadow.col = "gray")
pred <- predict(bfitpruned, bh[-t.id,])
sqrt(mean((pred - bh[-t.id,]$MEDV)^2))
preds <- predict(bfitpruned, bh[t.id,])
sqrt(mean((preds - bh[t.id,]$MEDV)^2))
preds <- predict(bfitpruned, bh[-t.id,])
sqrt(mean((preds - bh[-t.id,]$MEDV)^2))
preds <- predict(bfit, bh[t.id,])
sqrt(mean((preds - bh[t.id,]$MEDV)^2))
preds <- predict(bfit, bh[-t.id,])
sqrt(mean((preds - bh[-t.id,]$MEDV)^2))
library(rpart)
library(rpart.plot)
library(caret)
ed <- read.csv("Seccion 06 - Regresion/education.csv")
ed$region <- factor(ed$region)
View(ed)
t.id <- createDataPartition(ed$expense,
p = 0.7,
list = F)
fit <- rpart(expense ~ region+urban+income+under18,
data = ed[t.id,])
prp(fit,
type = 2,
nn = T,
fallen.leaves = T,
faclen = 4,
varlen = 8,
shadow.col = "gray")
#bagging
install.packages("ipred")
#bagging
install.packages("ipred")
library(ipred)
bagging.fit <- bagging(expense ~ region+urban+income+under18,
data = ed[t.id,])
prediction.t <- predict(bagging.fit, ed[t.id,])
sqrt(mean((prediction.t - ed[t.id,]$expense)^2))
pred <- predict(fit, ed[t.id,])
pred <- predict(fit, ed[t.id,])
sqrt(mean((pred - ed[t.id,]$expense)^2))
prp(bagging.fit,
type = 2,
nn = T,
fallen.leaves = T,
faclen = 4,
varlen = 8,
shadow.col = "gray")
#boosting
install.packages("gbm")
library(gbm)
data = ed[t.id,]
gbm.fit <- gbm(expense ~ region+urban+income+under18,
data = ed[t.id,])
gbm.fit <- gbm(expense ~ region+urban+income+under18,
data = ed[t.id,],
distribution = "gaussiana")
gbm.fit <- gbm(expense ~ region+urban+income+under18,
data = ed[t.id,],
distribution = "gaussian")
gbm.fit <- gbm(expense ~ region+urban+income+under18,
data = ed,
distribution = "gaussian")
library(randomForest)
library(caret)
bh <- read.csv("Seccion 06 - Regresion/BostonHousing.csv")
set.seed(2018)
t.id <- createDataPartition(bh$MEDV,
p = 0.7,
list = F)
mod <- randomForest(x = bh[t.id, 1:13],
y = bh[t.id, 14],
ntree = 1000,
xtest = bh[-t.id, 1:13],
ytest = bh[-t.id, 14],
importance = T,
keep.forest = T)
mod
mod$importance
plot(bh[t.id,]$MEDV,
predict(mod, bh[t.id,]),
xlab = "Actuales",
ylab = "Predichos")
abline(0,1)
plot(bh[-t.id,]$MEDV,
predict(mod, bh[-t.id,]),
xlab = "Actuales",
ylab = "Predichos")
abline(0,1)
