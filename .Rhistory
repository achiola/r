levels = c(3,5,6,8),
labels = c("3 cyl", "4 cyl", "5 cyl", "6 cyl", "8 cyl"))
auto$cylinders <- factor(auto$cylinders,
levels = c(3,4, 5,6,8),
labels = c("3 cyl", "4 cyl", "5 cyl", "6 cyl", "8 cyl"))
set.seed(2018)
t.id <- createDataPartition(auto$mpg,
p = 0.7,
list = F)
names(auto)
mod <- lm(mpg ~ .,
data = auto[t.id, -c(1, 8, 9)])
View(auto)
View(mod)
mod
summary(mod)
boxplot(mod$residuals)
sqrt(mean((mod$fitted.values - auto[t.id,]$mpg)^2))
sqrt(mean((mod$fitted.values - auto[t.id,]$mpg)^2))
pred <- predict(mod, auto[-t.id, -c(1,8,9)])
sqrt(mean((pred - auto[-t.id,]$mpg)^2))
pred
auto[-t.id,]$pred <- pred
auto$pred[-t.id,] <- pred
auto2 <- pred[-t.id,]
auto2 <- auto[-t.id,]
auto2$pred <- pred
View(auto2)
par(mfrow=c(2,2))
plot(mod)
auto <- within(auto, cylinders <- relevel(cylinders, ref="4 cyl"))
mod <- lm(mpg ~ .,
data = auto[t.id, -c(1, 8, 9)])
mod
pred <- predict(mod, auto[-t.id, -c(1,8,9)])
sqrt(mean((pred - auto[-t.id,]$mpg)^2))
plot(mod)
auto2$pred2 <- pred
#############################################################
# 81. La función step para simplificar el modelo lineal
#############################################################
library(MASS)
mod
summary(mod)
step.model <- stepAIC(mod, direction = "forward")
summary(step.model)
step.model <- stepAIC(mod, direction = "backward")
View(step.model)
install.packages(c("curl", "knitr", "recipes", "tidyr", "zip"))
install.packages("curl")
install.packages("curl")
install.packages("curl")
library(curl)
detach("package:curl", unload = TRUE)
install.packages("curl")
install.packages("rpart")
library(rpart)
install.packages("rpart.plot")
library(rpart.plot)
library(caret)
bh <- read.csv("Seccion 06 - Regresion/BostonHousing.csv")
View(bh)
t.id <- createDataPartition(bh$MEDV,
p = 0.7,
list = F)
bfit <- rpart(MEDV ~ .,
data = bh[t.id,])
bfit
prp(bfit,
type = 2,
nn = T,
fallen.leaves = T,
faclen = 4,
varlen = 8,
shadow.col = "gray")
bfit$cptable
plotcp(bfit)
0.2642192 + 0.04714776
bfitpruned <- prune(bfit, cp = 0.311367)
prp(bfitpruned,
type = 2,
nn = T,
fallen.leaves = T,
faclen = 4,
varlen = 8,
shadow.col = "gray")
bfitpruned <- prune(bfit, cp = 0.02357120)
prp(bfitpruned,
type = 2,
nn = T,
fallen.leaves = T,
faclen = 4,
varlen = 8,
shadow.col = "gray")
pred <- predict(bfitpruned, bh[-t.id,])
sqrt(mean((pred - bh[-t.id,]$MEDV)^2))
preds <- predict(bfitpruned, bh[t.id,])
sqrt(mean((preds - bh[t.id,]$MEDV)^2))
preds <- predict(bfitpruned, bh[-t.id,])
sqrt(mean((preds - bh[-t.id,]$MEDV)^2))
preds <- predict(bfit, bh[t.id,])
sqrt(mean((preds - bh[t.id,]$MEDV)^2))
preds <- predict(bfit, bh[-t.id,])
sqrt(mean((preds - bh[-t.id,]$MEDV)^2))
library(rpart)
library(rpart.plot)
library(caret)
ed <- read.csv("Seccion 06 - Regresion/education.csv")
ed$region <- factor(ed$region)
View(ed)
t.id <- createDataPartition(ed$expense,
p = 0.7,
list = F)
fit <- rpart(expense ~ region+urban+income+under18,
data = ed[t.id,])
prp(fit,
type = 2,
nn = T,
fallen.leaves = T,
faclen = 4,
varlen = 8,
shadow.col = "gray")
#bagging
install.packages("ipred")
#bagging
install.packages("ipred")
library(ipred)
bagging.fit <- bagging(expense ~ region+urban+income+under18,
data = ed[t.id,])
prediction.t <- predict(bagging.fit, ed[t.id,])
sqrt(mean((prediction.t - ed[t.id,]$expense)^2))
pred <- predict(fit, ed[t.id,])
pred <- predict(fit, ed[t.id,])
sqrt(mean((pred - ed[t.id,]$expense)^2))
prp(bagging.fit,
type = 2,
nn = T,
fallen.leaves = T,
faclen = 4,
varlen = 8,
shadow.col = "gray")
#boosting
install.packages("gbm")
library(gbm)
data = ed[t.id,]
gbm.fit <- gbm(expense ~ region+urban+income+under18,
data = ed[t.id,])
gbm.fit <- gbm(expense ~ region+urban+income+under18,
data = ed[t.id,],
distribution = "gaussiana")
gbm.fit <- gbm(expense ~ region+urban+income+under18,
data = ed[t.id,],
distribution = "gaussian")
gbm.fit <- gbm(expense ~ region+urban+income+under18,
data = ed,
distribution = "gaussian")
library(randomForest)
library(caret)
bh <- read.csv("Seccion 06 - Regresion/BostonHousing.csv")
set.seed(2018)
t.id <- createDataPartition(bh$MEDV,
p = 0.7,
list = F)
mod <- randomForest(x = bh[t.id, 1:13],
y = bh[t.id, 14],
ntree = 1000,
xtest = bh[-t.id, 1:13],
ytest = bh[-t.id, 14],
importance = T,
keep.forest = T)
mod
mod$importance
plot(bh[t.id,]$MEDV,
predict(mod, bh[t.id,]),
xlab = "Actuales",
ylab = "Predichos")
abline(0,1)
plot(bh[-t.id,]$MEDV,
predict(mod, bh[-t.id,]),
xlab = "Actuales",
ylab = "Predichos")
abline(0,1)
library(nnet)
library(caret)
library(devtools)
install.packages("devtools")
library(devtools)
bh <- read.csv("Seccion 06 - Regresion/BostonHousing.csv")
set.seed(2018)
t.id <- createDataPartition(bh$MEDV,
p = 0.7,
list = F)
summary(bh$MEDV)
#vamos a escalar el medv
fit <- nnet(bh$MEDV ~ . / 50,
data = bh[t.id,],
size = 6,
decay = 0.1,
maxit = 1000,
linout = T)
#vamos a escalar el medv
fit <- nnet(bh$MEDV / 50 ~ .,
data = bh[t.id,],
size = 6,
decay = 0.1,
maxit = 1000,
linout = T)
#vamos a escalar el medv
fit <- nnet(bh$MEDV / 50 ~ .,
data = bh[t.id,],
size = 6,
decay = 0.1,
maxit = 1000,
linout = T)
View(bh)
names(bh)
#vamos a escalar el medv
fit <- nnet(bh$MEDV / 50 ~ CRIM,
data = bh[t.id,],
size = 6,
decay = 0.1,
maxit = 1000,
linout = T)
#vamos a escalar el medv
fit <- nnet(bh$MEDV / 50 ~ ZN,
data = bh[t.id,],
size = 6,
decay = 0.1,
maxit = 1000,
linout = T)
#vamos a escalar el medv
fit <- nnet(bh$MEDV/50 ~ .,
data = bh[t.id,],
size = 6,
decay = 0.1,
maxit = 1000,
linout = T)
#vamos a escalar el medv
fit <- nnet(bh$MEDV/50 ~ .,
data = bh[t.id,],
size = 4,
decay = 0.1,
maxit = 1000,
linout = T)
#vamos a escalar el medv
fit <- nnet(bh$MEDV/50 ~ .,
data = bh[t.id,],
size = 6,
MaxNWts = 1000,
decay = 0.1,
maxit = 1000,
linout = T)
#vamos a escalar el medv
fit <- nnet(bh$MEDV/50 ~ .,
data = bh[t.id,],
size = 6,
MaxNWts = 10000,
decay = 0.1,
maxit = 1000,
linout = T)
#vamos a escalar el medv
fit <- nnet(bh$MEDV/50 ~ .,
data = bh[t.id,],
size = 6,
MaxNWts = 100000,
decay = 0.1,
maxit = 1000,
linout = T)
#vamos a escalar el medv
fit <- nnet(MEDV/50 ~ .,
data = bh[t.id,],
size = 6,
decay = 0.1,
maxit = 1000,
linout = T)
source_url("https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r")
plot(fit, max.sp = T)
sqrt(mean((fit$fitted.values*50-bh[t.id,"MEDV"])^2))
pred <- predict(fit, bh[-t.id, ])
predictions <- bh[-t.id, ]
predictions$pred <- pred
View(predictions)
predictions$pred <- pred*50
sqrt(mean((pred*50-bh[t.id,"MEDV"])^2))
sqrt(mean((pred*50-bh[-t.id,"MEDV"])^2))
bh <- read.csv("Seccion 06 - Regresion/BostonHousing.csv")
kfold.crossval.reg <- function(df, nfolds){
fold <- sample(1:nfolds,
nrow(df),
replace = T)
mean.square.errs <- sapply(1:nfolds,
kfold.crossval.reg.iter,
df,
fold)
list("MSE" = mean.square.errs,
"Overall_Mean_Sqr_Error" = mean(mean.square.errs),
"Std_Mean_Sqr_Error" = sd(mean.square.errs))
}
kfold.crossval.reg.iter <- function(k, df, fold){
t.id <- !fold %in% c(k)
test.id <- fold %in% c(k)
mod <- lm(MEDV ~ .,
data = df[t.id,])
preds <- predict(mod, df[test.id,])
sqr_errs <- (preds - df[test.id,"MEDV"])^2
mean(sqr_errs)
}
res <- kfold.crossval.reg(bh, 5)
res
kfold.crossval.reg.iter <- function(k, df, fold){
t.id <- !fold %in% c(k)
cat("----------------------------")
cat(t.id)
test.id <- fold %in% c(k)
mod <- lm(MEDV ~ .,
data = df[t.id,])
preds <- predict(mod, df[test.id,])
sqr_errs <- (preds - df[test.id,"MEDV"])^2
mean(sqr_errs)
}
res <- kfold.crossval.reg(bh, 5)
kfold.crossval.reg.iter <- function(k, df, fold){
t.id <- !fold %in% c(k)
test.id <- fold %in% c(k)
mod <- lm(MEDV ~ .,
data = df[t.id,])
preds <- predict(mod, df[test.id,])
sqr_errs <- (preds - df[test.id,"MEDV"])^2
mean(sqr_errs)
}
res <- kfold.crossval.reg(bh, 5)
res
res <- kfold.crossval.reg(bh, 10)
res
res <- kfold.crossval.reg(bh, 10)
res
res <- kfold.crossval.reg(bh, 100)
res
res <- kfold.crossval.reg(bh, 4)
res
res <- kfold.crossval.reg(bh, 5)
res
# 87. Implementando una LOOCV en R
loocv.reg <- function(df){
mean.sqr.errs <- sapply(1:NROW(df),
loocv.reg.iter,
df)
list("MSE" = mean.square.errs,
"Overall_Mean_Sqr_Error" = mean(mean.square.errs),
"Std_Mean_Sqr_Error" = sd(mean.square.errs))
}
loocv.reg.iter <- function(k, df){
mod <- lm(MEDV ~ .,
data = df[-k,])
pred <- predict(mod, df[k,])
sqr.error <- (pred - df[k,"MEDV"])^2
sqr.error
}
res <- loocv.reg(df)
# 87. Implementando una LOOCV en R
loocv.reg <- function(df){
mean.sqr.errs <- sapply(1:nrow(df),
loocv.reg.iter,
df)
list("MSE" = mean.square.errs,
"Overall_Mean_Sqr_Error" = mean(mean.square.errs),
"Std_Mean_Sqr_Error" = sd(mean.square.errs))
}
res <- loocv.reg(df)
df
res <- loocv.reg(bh)
# 87. Implementando una LOOCV en R
loocv.reg <- function(df){
mean.square.errs <- sapply(1:nrow(df),
loocv.reg.iter,
df)
list("MSE" = mean.square.errs,
"Overall_Mean_Sqr_Error" = mean(mean.square.errs),
"Std_Mean_Sqr_Error" = sd(mean.square.errs))
}
res <- loocv.reg(bh)
res
protein <- read.csv("Seccion 07 - Técnicas de reduccion de datos/protein.csv")
View(protein)
# normalizar y escalar
data <- as.data.frame(scale(protein[,-1]))
View(data)
data$Country <- protein$Country
#clustering aglomerativo
hc <- hclust(dist(data,
method = "euclidean"),
method = "ward.D2")
hc
plot(hc,
hang = 0.01,
cex = 0.7)
rownames(data) <- data$Country
#clustering aglomerativo
hc <- hclust(dist(data,
method = "euclidean"),
method = "ward.D2")
hc
plot(hc,
hang = 0.01,
cex = 0.7)
#single
hc2 <- hclust(dist(data,
method = "euclidean"),
method = "single")
hc2
plot(hc2,
hang = 0.01,
cex = 0.7)
#90. Las distancias y el método de generación del cluster
d <- dist(data, method = "euclidean")
d
Albania <- data[1,-1]
View(Albania)
Albania <- data["Albania",-1]
View(Albania)
Austria <- data["Austria", -1]
Albania <- data["Albania",-10]
Austria <- data["Austria", -10]
Albania - Austria
(Albania - Austria)^2
sqr(sum((Albania - Austria)^2))
sqrt(sum((Albania - Austria)^2))
d <- dist(data, method = "manhattan")
d
sum(abs(Albania - Austria))
#complete
hc3 <- hclust(dist(data,
method = "euclidean"),
method = "complete")
hc3
plot(hc3,
hang = 0.01,
cex = 0.7)
#average
hc4 <- hclust(dist(data,
method = "euclidean"),
method = "average")
hc4
plot(hc4,
hang = 0.01,
cex = 0.7)
hc4$merge
#91. Clusterings divisitivos y cortes en el dendograma
install.packages("cluster")
library(cluster)
dv <- diana(data, metric = "euclidean")
plot(dv)
par(nfrow=(2,1))
par(mfrow=(2,1))
par(mfrow=c(2,1))
plot(dv)
par(mfrow=c(1,2))
plot(dv)
par(mfrow=c(1,1))
#Cuttree
#single
hc2 <- hclust(dist(data,
method = "euclidean"),
method = "single")
hc2
plot(hc2,
hang = 0.01,
cex = 0.7)
#Cuttree
#single
hc2 <- hclust(dist(data,
method = "euclidean"),
method = "single")
hc2
plot(hc2,
hang = 0.01,
cex = 0.7)
#Cuttree
#ward.D2
hc <- hclust(dist(data,
method = "euclidean"),
method = "ward.D2")
hc
plot(hc,
hang = 0.01,
cex = 0.7)
cutree(hc, k=4)
fit <- cutree(hc, k=4)
table(fit)
rect.hclust(hc, k=4, border = "red")
rect.hclust(hc, k=3, border = "blue")
protein <- read.csv("Seccion 07 - Técnicas de reduccion de datos/protein.csv")
rownames(protein) <- protein$Country
protein$Country <- NULL
protein.scale <- as.data.frame(scale(protein))
View(protein)
View(protein.scale)
library(devtools)
devtools::install_github("kassambara/factoextra")
devtools::install_github("kassambara/factoextra")
install.packages(c("curl", "htmlTable", "mgcv", "pkgconfig", "raster"))
install.packages("curl")
protein <- read.csv("Seccion 07 - Técnicas de reduccion de datos/protein.csv")
rownames(protein) <- protein$Country
protein$Country <- NULL
protein.scale <- as.data.frame(scale(protein))
library(devtools)
library(devtools)
install.packages("digest")
library(devtools)
devtools::install_github("kassambara/factoextra")
devtools::install_github("kassambara/factoextra")
