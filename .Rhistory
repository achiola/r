res
kfold.crossval.reg.iter <- function(k, df, fold){
t.id <- !fold %in% c(k)
cat("----------------------------")
cat(t.id)
test.id <- fold %in% c(k)
mod <- lm(MEDV ~ .,
data = df[t.id,])
preds <- predict(mod, df[test.id,])
sqr_errs <- (preds - df[test.id,"MEDV"])^2
mean(sqr_errs)
}
res <- kfold.crossval.reg(bh, 5)
kfold.crossval.reg.iter <- function(k, df, fold){
t.id <- !fold %in% c(k)
test.id <- fold %in% c(k)
mod <- lm(MEDV ~ .,
data = df[t.id,])
preds <- predict(mod, df[test.id,])
sqr_errs <- (preds - df[test.id,"MEDV"])^2
mean(sqr_errs)
}
res <- kfold.crossval.reg(bh, 5)
res
res <- kfold.crossval.reg(bh, 10)
res
res <- kfold.crossval.reg(bh, 10)
res
res <- kfold.crossval.reg(bh, 100)
res
res <- kfold.crossval.reg(bh, 4)
res
res <- kfold.crossval.reg(bh, 5)
res
# 87. Implementando una LOOCV en R
loocv.reg <- function(df){
mean.sqr.errs <- sapply(1:NROW(df),
loocv.reg.iter,
df)
list("MSE" = mean.square.errs,
"Overall_Mean_Sqr_Error" = mean(mean.square.errs),
"Std_Mean_Sqr_Error" = sd(mean.square.errs))
}
loocv.reg.iter <- function(k, df){
mod <- lm(MEDV ~ .,
data = df[-k,])
pred <- predict(mod, df[k,])
sqr.error <- (pred - df[k,"MEDV"])^2
sqr.error
}
res <- loocv.reg(df)
# 87. Implementando una LOOCV en R
loocv.reg <- function(df){
mean.sqr.errs <- sapply(1:nrow(df),
loocv.reg.iter,
df)
list("MSE" = mean.square.errs,
"Overall_Mean_Sqr_Error" = mean(mean.square.errs),
"Std_Mean_Sqr_Error" = sd(mean.square.errs))
}
res <- loocv.reg(df)
df
res <- loocv.reg(bh)
# 87. Implementando una LOOCV en R
loocv.reg <- function(df){
mean.square.errs <- sapply(1:nrow(df),
loocv.reg.iter,
df)
list("MSE" = mean.square.errs,
"Overall_Mean_Sqr_Error" = mean(mean.square.errs),
"Std_Mean_Sqr_Error" = sd(mean.square.errs))
}
res <- loocv.reg(bh)
res
protein <- read.csv("Seccion 07 - Técnicas de reduccion de datos/protein.csv")
View(protein)
# normalizar y escalar
data <- as.data.frame(scale(protein[,-1]))
View(data)
data$Country <- protein$Country
#clustering aglomerativo
hc <- hclust(dist(data,
method = "euclidean"),
method = "ward.D2")
hc
plot(hc,
hang = 0.01,
cex = 0.7)
rownames(data) <- data$Country
#clustering aglomerativo
hc <- hclust(dist(data,
method = "euclidean"),
method = "ward.D2")
hc
plot(hc,
hang = 0.01,
cex = 0.7)
#single
hc2 <- hclust(dist(data,
method = "euclidean"),
method = "single")
hc2
plot(hc2,
hang = 0.01,
cex = 0.7)
#90. Las distancias y el método de generación del cluster
d <- dist(data, method = "euclidean")
d
Albania <- data[1,-1]
View(Albania)
Albania <- data["Albania",-1]
View(Albania)
Austria <- data["Austria", -1]
Albania <- data["Albania",-10]
Austria <- data["Austria", -10]
Albania - Austria
(Albania - Austria)^2
sqr(sum((Albania - Austria)^2))
sqrt(sum((Albania - Austria)^2))
d <- dist(data, method = "manhattan")
d
sum(abs(Albania - Austria))
#complete
hc3 <- hclust(dist(data,
method = "euclidean"),
method = "complete")
hc3
plot(hc3,
hang = 0.01,
cex = 0.7)
#average
hc4 <- hclust(dist(data,
method = "euclidean"),
method = "average")
hc4
plot(hc4,
hang = 0.01,
cex = 0.7)
hc4$merge
#91. Clusterings divisitivos y cortes en el dendograma
install.packages("cluster")
library(cluster)
dv <- diana(data, metric = "euclidean")
plot(dv)
par(nfrow=(2,1))
par(mfrow=(2,1))
par(mfrow=c(2,1))
plot(dv)
par(mfrow=c(1,2))
plot(dv)
par(mfrow=c(1,1))
#Cuttree
#single
hc2 <- hclust(dist(data,
method = "euclidean"),
method = "single")
hc2
plot(hc2,
hang = 0.01,
cex = 0.7)
#Cuttree
#single
hc2 <- hclust(dist(data,
method = "euclidean"),
method = "single")
hc2
plot(hc2,
hang = 0.01,
cex = 0.7)
#Cuttree
#ward.D2
hc <- hclust(dist(data,
method = "euclidean"),
method = "ward.D2")
hc
plot(hc,
hang = 0.01,
cex = 0.7)
cutree(hc, k=4)
fit <- cutree(hc, k=4)
table(fit)
rect.hclust(hc, k=4, border = "red")
rect.hclust(hc, k=3, border = "blue")
protein <- read.csv("Seccion 07 - Técnicas de reduccion de datos/protein.csv")
rownames(protein) <- protein$Country
protein$Country <- NULL
protein.scale <- as.data.frame(scale(protein))
View(protein)
View(protein.scale)
library(devtools)
devtools::install_github("kassambara/factoextra")
devtools::install_github("kassambara/factoextra")
install.packages(c("curl", "htmlTable", "mgcv", "pkgconfig", "raster"))
install.packages("curl")
protein <- read.csv("Seccion 07 - Técnicas de reduccion de datos/protein.csv")
rownames(protein) <- protein$Country
protein$Country <- NULL
protein.scale <- as.data.frame(scale(protein))
library(devtools)
library(devtools)
install.packages("digest")
library(devtools)
devtools::install_github("kassambara/factoextra")
devtools::install_github("kassambara/factoextra")
protein <- read.csv("Seccion 07 - Técnicas de reduccion de datos/protein.csv")
rownames(protein) <- protein$Country
protein$Country <- NULL
protein.scale <- as.data.frame(scale(protein))
library(devtools)
devtools::install_github("kassambara/factoextra")
km <- kmeans(protein.scale, k = 4)
km <- kmeans(protein.scale, 4)
km
aggregate(protein.scale, by = list(cluster = km$cluster))
aggregate(protein.scale, by = list(cluster = km$cluster), mean)
library(factoextra)
fviz_cluster(km, data = protein.scale)
install.packages("OpenImageR")
install.packages("ClusterR")
library(OpenImageR)
library(ClusterR)
library(OpenImageR)
library(ClusterR)
img <- readImage("Seccion 07 - Técnicas de reduccion de datos/bird.jpg")
library(OpenImageR)
library(ClusterR)
img <- readImage("Seccion 07 - Tecnicas de reduccion de datos/bird.jpg")
img.resize <- resizeImage(img, 350, 350, method = "bilinear")
imageShow(img.resize)
img.vector <- apply(img.resize , 3, as.vector)
dim(img.vector)
kmmb <- MiniBatchKmeans(img.vector,
clusters = 5,
batch_size = 20,
num_init = 5,
max_iters = 100,
init_fraction = 0.2,
initializer = "kmeans++",
early_stop_iter = 10,
verbose = F)
prmb <- predict_MBatchKMeans(img.vector,
kmmb$centroids)
get.cent.mb <- kmmb$centroids
new.img <- get.cent.mb[prmb,]
dim(new.img) <- c(nrow(img.resize), ncol(img.resize), 3)
imageShow(new.img)
######## Generalizacion, con imagen de bodegon
img.name = "Seccion 07 - Tecnicas de reduccion de datos/bodegon.jpg"
img <- readImage(img.name)
imageShow(img.resize)
img <- readImage(img.name)
imageShow(img.resize)
imageShow(img)
imageShow(img)
img.vector <- apply(img , 3, as.vector)
dim(img.vector)
kmmb <- MiniBatchKmeans(img.vector,
clusters = 5,
batch_size = 20,
num_init = 5,
max_iters = 100,
init_fraction = 0.2,
initializer = "kmeans++",
early_stop_iter = 10,
verbose = F)
prmb <- predict_MBatchKMeans(img.vector,
kmmb$centroids)
get.cent.mb <- kmmb$centroids
new.img <- get.cent.mb[prmb,]
dim(new.img) <- c(nrow(img.resize), ncol(img.resize), 3)
dim(new.img) <- c(nrow(img), ncol(img), 3)
imageShow(new.img)
kmmb <- MiniBatchKmeans(img.vector,
clusters = 5,
batch_size = 20,
num_init = 5,
max_iters = 100,
init_fraction = 0.2,
initializer = "kmeans++",
early_stop_iter = 10,
verbose = F)
kmmb <- MiniBatchKmeans(img.vector,
clusters = 6,
batch_size = 20,
num_init = 5,
max_iters = 100,
init_fraction = 0.2,
initializer = "kmeans++",
early_stop_iter = 10,
verbose = F)
prmb <- predict_MBatchKMeans(img.vector,
kmmb$centroids)
get.cent.mb <- kmmb$centroids
new.img <- get.cent.mb[prmb,]
dim(new.img) <- c(nrow(img), ncol(img), 3)
imageShow(new.img)
######## imagen del muñeco
img.name = "Seccion 07 - Tecnicas de reduccion de datos/jb.jpg"
img <- readImage(img.name)
imageShow(img)
img.vector <- apply(img , 3, as.vector)
dim(img.vector)
kmmb <- MiniBatchKmeans(img.vector,
clusters = 6,
batch_size = 20,
num_init = 5,
max_iters = 100,
init_fraction = 0.2,
initializer = "kmeans++",
early_stop_iter = 10,
verbose = F)
kmmb <- MiniBatchKmeans(img.vector,
clusters = 5,
batch_size = 20,
num_init = 5,
max_iters = 100,
init_fraction = 0.2,
initializer = "kmeans++",
early_stop_iter = 10,
verbose = F)
prmb <- predict_MBatchKMeans(img.vector,
kmmb$centroids)
get.cent.mb <- kmmb$centroids
new.img <- get.cent.mb[prmb,]
dim(new.img) <- c(nrow(img), ncol(img), 3)
imageShow(new.img)
kmmb <- MiniBatchKmeans(img.vector,
clusters = 6,
batch_size = 20,
num_init = 5,
max_iters = 100,
init_fraction = 0.2,
initializer = "kmeans++",
early_stop_iter = 10,
verbose = F)
prmb <- predict_MBatchKMeans(img.vector,
kmmb$centroids)
get.cent.mb <- kmmb$centroids
new.img <- get.cent.mb[prmb,]
dim(new.img) <- c(nrow(img), ncol(img), 3)
imageShow(new.img)
protein <- read.csv("Seccion 07 - Tecnicas de reduccion de datos/protein.csv")
rownames(protein) <- protein$Country
protein$Country <- NULL
protein.scaled <- as.data.frame(scale(protein))
library(devtools)
library(factoextra)
km <- kmeans(protein.scaled, 4)
library(cluster)
library(factoextra)
km <- pam(protein.scaled, 4)
km
fviz_cluster(km)
## 95. Clustering large application (clara)
clarafit <- clara(protein.scaled, 4, samples = 5)
clarafit
fviz_cluster(clarafit)
install.packages("fpc")
install.packages("NbClust")
library(factoextra)
library(cluster)
library(fpc)
library(NbClust)
protein <- read.csv("Seccion 07 - Tecnicas de reduccion de datos/protein.csv")
rownames(protein) <- protein$Country
protein$Country <- NULL
protein.scaled <- as.data.frame(scale(protein))
nb <- NbClust(protein.scaled,
distance = "euclidean",
min.nc = 2,
max.nc = 12,
method = "ward.D2",
index = "all")
fviz_nbclust(nb) + theme_minimal()
km.res <- kmeans(protein.scaled, 3)
km.sil <- silhouette(km.res$cluster, dist(protein.scaled))
sil.sum <- summary(km.sil)
sil.sum
fviz_silhouette(km.sil)
dd <- dist(protein.scaled, method = "euclidean")
pam_stats <- cluster.stats(dd, km.res$cluster)
pam_stats
km
km_stats <- cluster.stats(dd, km.res$cluster)
km_stats
km_stats
km_stats$within.cluster.ss
km_stats$clus.avg.silwidths
km_stats$dunn
kmed <- pam(protein.scaled, 3)
kmed.stats <- cluster.stats(dd, kmed)
kmed.stats <- cluster.stats(dd, kmed$clustering)
kmed.stats$dunn
kmed_stats$within.cluster.ss
kmed.stats$within.cluster.ss
km.stats$clus.avg.silwidths
kmed.stats$clus.avg.silwidths
km.sil <- silhouette(kmed$clustering, dist(protein.scaled))
kmed.sil <- silhouette(kmed$clustering, dist(protein.scaled))
fviz_silhouette(kmed.sil)
km.sil <- silhouette(km.res$cluster, dist(protein.scaled))
fviz_cluster(km.res, data = protein.scaled)
fviz_cluster(kmed, data = protein.scaled)
fviz_cluster(km.res, data = protein.scaled)
fviz_cluster(kmed, data = protein.scaled)
fviz_cluster(km.res, data = protein.scaled)
fviz_cluster(kmed, data = protein.scaled)
res.com <- cluster.stats(dd, km.res$cluster, kmed$clustering)
res.com$corrected.rand
res.com$vi
library(fpc)
library(factoextra)
data("multishapes", package = "factuextra")
data("multishapes", package = "factoextra")
force(multishapes)
dataPoints <- multishapes[,1:2]
View(multishapes)
View(dataPoints)
plot(dataPoints)
km <- kmeans(dataPoints, 5)
fviz_cluster(km, dataPoints)
dsFit <- dbscan(dataPoints,
eps = 0.15,
MinPts = 5)
dsFit
fviz_cluster(dsFit, dataPoints)
fviz_cluster(dsFit,
dataPoints,
geom = "point")
#98. Clusterings basados en modelos
install.packages("mclust")
#98. Clusterings basados en modelos
install.packages("mclust")
library(mclust)
mclust <- Mclust(dataPoints)
plot(mclust)
summary(mclust)
bh <- read.csv("Seccion 07 - Tecnicas de reduccion de datos/BostonHousing.csv")
install.packages("corrplot")
library(corrplot)
corr <- cor(bh[,-14])
corr
corrplot(corr,
method = "color")
corrplot(corr,
method = "circle")
bh.acp <- prcomp(bh[,-14],
scale. = T)
summary(bh.acp)
plot(bh.acp)
plot(bh.acp, type = "lines")
biplot(bh.acp, col("gray", "red"))
biplot(bh.acp, c("gray", "red"))
biplot(bh.acp, col = c("gray", "red"))
head(bh.acp$x,5)
bh.acp$rotation
bh.acp$sdev
AMZN <- read.csv("Seccion 08 - Series temporales/AMZN.csv")
View(AMZN)
AMZN <- read.csv("Seccion 08 - Series temporales/AMZN.csv", stringsAsFactors = F)
View(AMZN)
APPL <- read.csv("Seccion 08 - Series temporales/AAPL.csv", stringsAsFactors = F)
FB <- read.csv("Seccion 08 - Series temporales/FB.csv", stringsAsFactors = F)
GOOG <- read.csv("Seccion 08 - Series temporales/GOOG.csv", stringsAsFactors = F)
head(AMZN)
AMZN <- AMZN[AMZN$Date>='2008-01-01',]
head(AMZN)
APPL <- APPL[APPL$Date>='2008-01-01',]
GOOG <- GOOG[GOOG$Date>='2008-01-01',]
str(APPL)
APPL$Date <- as.Date(APPL$Date)
str(APPL)
FB$Date <- as.Date(FB$Date)
GOOG$Date <- as.Date(GOOG$Date)
library(ggplot2)
ggplot(APPL,
aes(Date, close)) +
geom_line(aes(color="Apple")) +
geom_line(data = AMZN, aes(color = "Amazon")) +
geom_line(data = FB, aes(color = "Facebook")) +
geom_line(data = GOOG, aes(color = "Google")) +
labs(color = "Legend") +
scale_color_manual("", breaks = c("Apple","Amazon","Facebook","Google"),
values = c("gray", "yellow","blue", "red")) +
ggtitle("Comparaciones de cierre de stocks") +
theme(plot.title = element_text(lineheight = 0.7, face = "bold"))
ggplot(APPL, aes(Date, Close)) +
geom_line(aes(color="Apple")) +
geom_line(data = AMZN, aes(color = "Amazon")) +
geom_line(data = FB, aes(color = "Facebook")) +
geom_line(data = GOOG, aes(color = "Google")) +
labs(color = "Legend") +
scale_color_manual("", breaks = c("Apple","Amazon","Facebook","Google"),
values = c("gray", "yellow","blue", "red")) +
ggtitle("Comparaciones de cierre de stocks") +
theme(plot.title = element_text(lineheight = 0.7, face = "bold"))
View(AMZN)
APPL$Date <- as.Date(APPL$Date)
AMZN$Date <- as.Date(AMZN$Date)
FB$Date <- as.Date(FB$Date)
GOOG$Date <- as.Date(GOOG$Date)
library(ggplot2)
ggplot(APPL, aes(Date, Close)) +
geom_line(aes(color="Apple")) +
geom_line(data = AMZN, aes(color = "Amazon")) +
geom_line(data = FB, aes(color = "Facebook")) +
geom_line(data = GOOG, aes(color = "Google")) +
labs(color = "Legend") +
scale_color_manual("", breaks = c("Apple","Amazon","Facebook","Google"),
values = c("gray", "yellow","blue", "red")) +
ggtitle("Comparaciones de cierre de stocks") +
theme(plot.title = element_text(lineheight = 0.7, face = "bold"))
