validation[,4:5],
train[,3],
k = 1)
tab <- table(validation$Result,
pred1,
dnn = c("Actual", "Predicho"))
summary(tab)
confusionMatrix(tab)
tab
# va a decidir solo el vecino mas cercano
pred1 <- knn(train[,4:5],
validation[,4:5],
train[,3],
k = 1)
tab1 <- table(validation$Result,
pred1,
dnn = c("Actual", "Predicho"))
tab1
# ahora con el voto de dos vecinos cercanos
pred2 <- knn(train[,4:5],
validation[,4:5],
train[,3],
k = 2)
tab2 <- table(validation$Result,
pred2,
dnn = c("Actual", "Predicho"))
tab2
# ahora con el voto de dos vecinos cercanos
pred2 <- knn(train[,4:5],
validation[,4:5],
train[,3],
k = 2)
tab2 <- table(validation$Result,
pred2,
dnn = c("Actual", "Predicho"))
tab2
# con 5
pred5 <- knn(train[,4:5],
validation[,4:5],
train[,3],
k = 5)
tab5 <- table(validation$Result,
pred5,
dnn = c("Actual", "Predicho"))
tab5
# con el conjunto de testing
pred.t <- knn(train[,4:5],
test[,4:5],
train[,3],
k = 5)
tab.t <- table(test$Result,
pred.t,
dnn = c("Actual", "Predicho"))
tab5
plot(pred.t)
#################################################################
## 67. Eligiendo el mejor número de vecinos para la decisión
#################################################################
knn.automate <- function(tr_predictors, val_predictors,
tr_target, val_target,
start_k, end_k){
for(k in start_k:end_k){
pred <- knn(tr_predictors,
val_predictors,
tr_target,
k)
tab <- table(val_target, pred, dnn = c("Actual", "Predicho"))
cat(paste("Matriz de confusion para k = ",k,"\n"))
cat("================================================\n")
cat(tab)
cat("------------------------------------------------\n")
}
}
knn.automate(train[,4:5],
validation[,4:5],
train[,3],
validation[,3],
1, 8
)
#################################################################
## 67. Eligiendo el mejor número de vecinos para la decisión
#################################################################
knn.automate <- function(tr_predictors, val_predictors,
tr_target, val_target,
start_k, end_k){
for(k in start_k:end_k){
pred <- knn(tr_predictors,
val_predictors,
tr_target,
k)
tab <- table(val_target, pred, dnn = c("Actual", "Predicho"))
cat(paste("Matriz de confusion para k = ",k,"\n"))
cat("================================================\n")
cat(tab)
cat("\n------------------------------------------------\n")
}
}
knn.automate(train[,4:5],
validation[,4:5],
train[,3],
validation[,3],
1, 8
)
#################################################################
## 67. Eligiendo el mejor número de vecinos para la decisión
#################################################################
knn.automate <- function(tr_predictors, val_predictors,
tr_target, val_target,
start_k, end_k){
for(k in start_k:end_k){
pred <- knn(tr_predictors,
val_predictors,
tr_target,
k)
tab <- table(val_target, pred, dnn = c("Actual", "Predicho"))
cat(paste("Matriz de confusion para k = ",k,"\n"))
cat("================================================\n")
print(tab)
cat("\n------------------------------------------------\n")
}
}
knn.automate(train[,4:5],
validation[,4:5],
train[,3],
validation[,3],
1, 8
)
tctrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3)
caret_knn_fit <- train(Result ~ Family_size + Income,
data = train,
method = "knn",
trControl = trControl,
preProcess = c("center", "scale"),
tuneLength = 10)
caret_knn_fit <- train(Result ~ Family_size + Income,
data = train,
method = "knn",
trControl = tctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
caret_knn_fit
pred6 <- knn(train[,4:5],
validation[,4:5],
train[,3],
k=5,
prob = T)
pred6
install.packages("nnet")
install.packages("nnet")
install.packages("zip")
library(nnet)
library(caret)
bn <- read.csv("Seccion 05 - El proceso de clasificacion/banknote-authentication.csv")
View(bn)
bn$class <- factor(bn$class)
View(bn)
t.id <- createDataPartition(bn$class,
p = 0.7,
list = F)
mod <- nnet(class ~ .,
data = bn[t.id,],
size = 3,
maxit = 10000,
decay = 0.001,
rang = 0.05)
pred <- predict(mod,
newdata = bn[-t.id],
type = "class")
pred <- predict(mod,
newdata = bn[-t.id],
type = "class")
pred <- predict(mod,
newdata = bn[-t.id,],
type = "class")
table(bn[-t.id,]$class,
pred,
dnn = c("Actual", "Predichos"))
pred <- predict(mod,
newdata = bn[-t.id,],
type = "class",
na.action = na.omit )
pred2 <- predict(mod,
newdata = bn[-t.id,],
type = "raw")
library(ROCR)
perf <- performance(prediction(pred2, bn[-t.id,"class"]),
"tpr",
"fpr")
plot(perf)
install.packages("MASS")
library(MASS)
library(caret)
bn <- read.csv("Seccion 05 - El proceso de clasificacion/banknote-authentication.csv")
View(bn)
bn$class <- factor(bn$class)
set.seed(2018)
t.id <- createDataPartition(bn$class,
p = 0.7,
list = F)
mod <- lda(bn[t.id,1:4],
bn[t.id,5])
bn[t.id,"predic"] <- predict(mod,
bn[t.id,1:4])$class
table(bn[t.id, "class"],
bn[t.id, "predic"],
dnn = c("actual", "predichos"))
bn[-t.id,"predic"] <- predict(mod,
bn[-t.id,1:4])$class
table(bn[t.id, "class"],
bn[t.id, "predic"],
dnn = c("actual", "predichos"))
table(bn[-t.id, "class"],
bn[-t.id, "predic"],
dnn = c("actual", "predichos"))
# el de abajo es identico al de arriba
mod <- lda(class ~ .,
data = bn[t.id,])
library(caret)
bh <- read.csv("Seccion 05 - El proceso de clasificacion/boston-housing-logistic.csv")
View(bh)
bh$CLASS <- factor(bh$CLASS)
View(bh)
set.seed(2018)
mod <- glm(CLASS ~ .,
data = bh[t.id,],
family = binomial)
t.id <- createDataPartition(bh$CLASS,
p = 0.7,
list = F)
mod <- glm(CLASS ~ .,
data = bh[t.id,],
family = binomial)
summary(mod)
bh[-t.id, "predic"] <- predict(mod,
newdata = bh[-t.id],
type = "response")
bh[-t.id, "predic"] <- predict(mod,
newdata = bh[-t.id,],
type = "response")
bh[-t.id, "predic"] <- NULL
bh["predic"] <- NULL
bh[-t.id, "prob_success"] <- predict(mod,
newdata = bh[-t.id,],
type = "response")
bh[-t.id, "pred_50"] <- ifelse(bh[-t.id,"prob_success"] > 0.5, 1, 0)
table(bh[-t.id,"CLASS"],
bh[-t.id, "prob_50"],
dnn = c("Actual", "Predicho"))
table(bh[-t.id,"CLASS"],
bh[-t.id, "pred_50"],
dnn = c("Actual", "Predicho"))
d <- read.csv("Seccion 06 - 75. La raíz del error cuadrático medio/rmse.csv")
View(d)
sqrt(mean((d$price - d$pred)^2))
rmse <- sqrt(mean((d$price - d$pred)^2))
rmse
plot(d$price,
d$pred,
xlab = "Actual",
ylab = "Predicho")
abline(0,1)
rmse.f <- function(actual, predicted){
return(sqrt(mean((actual - predicted)^2)))
}
rmse.f(d$price, d$pred)
install.packages"FNN")
install.packages("FNN")
library(dummies)
library(FNN)
library(scales)
library(caret)
edu <- read.csv("Seccion 06 - Regresion/education.csv")
View(edu)
#variables dummies para region
dms <- dummy(edu$region, sep = "_")
View(dms)
cbind(edu,dms)
edu <- cbind(edu,dms)
View(edu)
#normalizamos..
edu$urban.s <- rescale(edu$urban)
edu$income.s <- rescale(edu$income)
edu$under18.s <- rescale(edu$under18)
set.seed(2018)
t.id <- createDataPartition(edu$expense,
p = 0.6,
list = F)
tr <- edu[t.id,]
temp <- edu[-t.id,]
v.id <- createDataPartition(temp$expense,
p = 0.5,
list = F)
val <- temp[v.id, ]
test <- temp[-v.id, ]
#Creamos varios modelos para ver cual es el mejor
reg1 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 1,
algorithm = "brute")
rmse1 <- sqrt(mean((reg1$pred - val$expense)^2))
rmse1
## k = 2
reg2 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 2,
algorithm = "brute")
rmse2 <- sqrt(mean((reg1$pred - val$expense)^2))
rmse2
## k = 1
reg1 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 1,
algorithm = "brute")
rmse1 <- sqrt(mean((reg1$pred - val$pred)^2))
rmse1
rmse1 <- sqrt(mean((reg1$pred - val$expense)^2))
rmse1
## k = 2
reg2 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 2,
algorithm = "brute")
rmse2 <- sqrt(mean((reg2$pred - val$expense)^2))
rmse2
## k = 3
reg3 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 3,
algorithm = "brute")
rmse3 <- sqrt(mean((reg3$pred - val$expense)^2))
rmse3
## k = 4
reg4 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 4,
algorithm = "brute")
rmse4 <- sqrt(mean((reg4$pred - val$expense)^2))
rmse4
errors = c(rmse1,rmse2,rmse3,rmse4)
plot(errors,
type = 'o',
xlab = "k",
ylab = "RSME")
## k = 1
reg1 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 1,
algorithm = "brute")
rmse1 <- sqrt(mean((val$expense - reg1$pred)^2))
rmse1
## k = 1
reg1 <- knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
k = 1,
algorithm = "brute")
rmse1 <- sqrt(mean((reg1$pred - val$expense)^2))
rmse1
#######################
reg.test <- knn.reg(tr[,7:12],
test[,7:12],
tr$expense,
k = 4,
algorithm = "brute")
rmse.test <- sqrt(mean((reg.test$pred - test$expense)^2))
rmse.test
#######################
reg.test <- knn.reg(tr[,7:12],
test[,7:12],
tr$expense,
k = 3,
algorithm = "brute")
rmse.test <- sqrt(mean((reg.test$pred - test$expense)^2))
rmse.test
#######################
reg.test <- knn.reg(tr[,7:12],
test[,7:12],
tr$expense,
k = 4,
algorithm = "brute")
rmse.test <- sqrt(mean((reg.test$pred - test$expense)^2))
rmse.test
t.id <- createDataPartition(edu$expense,
p = 0.7,
list = F)
tr <- edu[t.id, ]
val <- edu[-t.id, ]
reg <- knn.reg(tr[,7:12],
test = NULL,
tr$expense,
k = 4,
algorithm = "brute")
rmse <- sqrt(mean((reg$residuals)^2))
rmse
reg <- knn.reg(tr[,7:12],
test = NULL,
tr$expense,
k = 3,
algorithm = "brute")
rmse <- sqrt(mean((reg$residuals)^2))
rmse
reg <- knn.reg(tr[,7:12],
test = NULL,
tr$expense,
k = 4,
algorithm = "brute")
rmse <- sqrt(mean((reg$residuals)^2))
rmse
library(FNN)
##################################################
# creamos un fx para hacer las regresiones
##################################################
rdacb.knn.reg <- function(tr_predictors, val_predictors,
tr_target, val_target, k){
library(FNN)
res <- knn.reg(tr_predictors,
val_predictors,
tr_target,
k,
algorithm = "brute")
rmserror <- sqrt(mean((val_target-res$pred)^2))
cat(paste("RMSE para k =  ", toString(k) , ": ", rmserror, "\n", sep = ""))
}
rdacb.knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
1)
rdacb.knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
2)
rdacb.knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
3)
rdacb.knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
4)
rdacb.knn.multi <- function(tr_predictors, val_predictors,
tr_target, val_target, start_k, end_k){
rms_errors <- vector()
for(k in start_k:end_k){
rms_error <- rdacb.knn.reg(tr_predictors,
val_predictors,
tr_target,
val_target,
k)
rms_errors <- c(rms_errors, rms_error)
}
plot(rms_errors,
type = "o",
xlab = "k",
ylab = "RMSE")
}
rdacb.knn.multi(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
1,
5)
##################################################
# creamos un fx para hacer las regresiones
##################################################
rdacb.knn.reg <- function(tr_predictors, val_predictors,
tr_target, val_target, k){
library(FNN)
res <- knn.reg(tr_predictors,
val_predictors,
tr_target,
k,
algorithm = "brute")
rmserror <- sqrt(mean((val_target-res$pred)^2))
cat(paste("RMSE para k =  ", toString(k) , ": ", rmserror, "\n", sep = ""))
rmserror
}
rdacb.knn.reg(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
1)
rdacb.knn.multi(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
1,
5)
rdacb.knn.multi(tr[,7:12],
val[,7:12],
tr$expense,
val$expense,
1,
10)
df = data.frame(actual = test$expense,
pred = reg.test$pred)
View(df)
plt(df)
plot(df)
abline(0,1)
